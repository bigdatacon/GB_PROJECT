{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import imblearn\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pandas import DataFrame\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from numpy import dtype\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy import sparse\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge imbalanced-learn\n",
    "!conda install -c conda-forge/label/gcc7 imbalanced-learn\n",
    "!conda install -c conda-forge/label/cf201901 imbalanced-learn\n",
    "!conda install -c conda-forge imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_folder = 'Issue_MOCK_Data_v0 - Sheet1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count, total_cols = df.shape\n",
    "print(\"Dataframe contains {} observations across {} columns\".format(total_count, total_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('LABEL').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LABEL'] = df['LABEL'].fillna(value=\"NOT_DEFINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['REMEDIATION_SUMMARY'] = df['REMEDIATION_SUMMARY'].fillna(value=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LEGACY_ID'] = df['LEGACY_ID'].fillna(value=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ISSUE_TEXT'] = df['ISSUE_TITLE'] + df['ISSUE_SUMMARY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['REMEDIATION_SUMMARY'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['ADDED_BY_KATYA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = pd.get_dummies(df['LABEL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, encoded_labels], axis = 1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def concatenate_list_data(lst):\n",
    "    result = ''\n",
    "    for element in lst:\n",
    "        result += str(element)\n",
    "        result += ' '\n",
    "    return result\n",
    "\n",
    "def pre_process(text, legacy):\n",
    "\n",
    "    filtered_words = ['']*len(text)\n",
    "    stemmer = WordNetLemmatizer\n",
    "    \n",
    "    text.strip(\"/\")\n",
    "    legacy.strip(\"/\")\n",
    "\n",
    "    document = re.sub(legacy, '', str(text))\n",
    "    noisy_words = [\"IA-AUD-\", \"POL-\", \"FNM-\", \"CTL-\", \"-OE\", \"SOX-\", \"-CR-\", \"IA-\", \n",
    "                   \"-MO-\", \"-TC-\", \"-ERM-\", \"-CO-\", \"-AOR-\", \"-MF-\", \"-NR\", \"-CR-\", \n",
    "                   \"-CPM\", \"-OCA-\", \"-DER-\"]\n",
    "    for word in noisy_words:\n",
    "        document = re.sub(word,'',document)\n",
    "\n",
    "    document = re.sub(r'\\W', ' ', document)\n",
    "    document = re.sub(\"_\", \"\", document)\n",
    "    document = re.sub(\"-\", \"\", document)\n",
    "    document = ''.join([i for i in document if not i.isdigit()])\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    document = document.lower()\n",
    "\n",
    "    word_tokens = word_tokenize(document)\n",
    "    filtered_words = [w for w in word_tokens if not w in stop_words]\n",
    "    document = concatenate_list_data(filtered_words)\n",
    "\n",
    "    #document = document.split()\n",
    "\n",
    "    #document = [stemmer.lemmatize(word) for word in document]\n",
    "    #document = ' '.join(document)\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Mauris lacinia sapien quis libero.'\n",
    "legacy = ''\n",
    "\n",
    "text.strip(\"/\")\n",
    "legacy.strip(\"/\")\n",
    "\n",
    "document = re.sub(legacy, '', str(text))\n",
    "noisy_words = [\"IA-AUD-\", \"POL-\", \"FNM-\", \"CTL-\", \"-OE\", \"SOX-\", \"-CR-\", \"IA-\", \n",
    "               \"-MO-\", \"-TC-\", \"-ERM-\", \"-CO-\", \"-AOR-\", \"-MF-\", \"-NR\", \"-CR-\", \n",
    "               \"-CPM\", \"-OCA-\", \"-DER-\"]\n",
    "for word in noisy_words:\n",
    "    document = re.sub(word,'',document)\n",
    "\n",
    "document = re.sub(r'\\W', ' ', document)\n",
    "document = re.sub(\"_\", \"\", document)\n",
    "document = re.sub(\"-\", \"\", document)\n",
    "document = ''.join([i for i in document if not i.isdigit()])\n",
    "document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "document = re.sub(r'^b\\s+', '', document)\n",
    "document = document.lower()\n",
    "\n",
    "word_tokens = word_tokenize(document)\n",
    "filtered_words = [w for w in word_tokens if not w in stop_words]\n",
    "document = concatenate_list_data(filtered_words)\n",
    "\n",
    "#document = document.split()\n",
    "#print(document)\n",
    "#document = [stemmer.lemmatize(w) for w in document]\n",
    "#document = ' '.join(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df.ISSUE_TEXT\n",
    "#y = encoded_labels\n",
    "#X_train, X_test, y_train, y_test = train_test_split (\n",
    "#                                    X, y, test_size = 0.2, random_state = 42,\n",
    "#                                    stratify = encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.apply(lambda x: pre_process(x['ISSUE_TEXT'], x['LEGACY_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_text = ['']*len(df)\n",
    "for i in range(df.ISSUE_NUMBER.count()):\n",
    "    pre_processed_text[i] = pre_process(df.ISSUE_TEXT[i], df.LEGACY_ID[i])\n",
    "pre_processed_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df.LABEL)\n",
    "le.classes_\n",
    "normalized_label = le.transform(df.LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pre_processed_text\n",
    "#labels = ['BRR', 'CSR']\n",
    "#for label in labels:\n",
    "#    y = df[label]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['BRR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split (\n",
    "                                    X, y, test_size = 0.2, random_state = 42,\n",
    "                                    stratify = normalized_label)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(max_df = 0.85, stop_words = stopwords.words('english'))\n",
    "xtrain_tfidf = tfidf_vect.fit_transform(X_train).toarray()\n",
    "xtest_tfidf = tfidf_vect.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_tfidf.shape, xtest_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state = 42)\n",
    "sm_xtrain_tfidf, sm_train_y = sm.fit_sample(xtrain_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "best_val_score = 0\n",
    "best_clf = None\n",
    "for clf in [LogisticRegression, LinearSVC, SGDClassifier]:\n",
    "    clf_pipeline = Pipeline(\n",
    "                            [\n",
    "                                #(\"vectorizer\", TfidfVectorizer(max_df = 0.85, \n",
    "                            #                                stop_words = stopwords.words('english'))),\n",
    "                                (\"classifier\", clf())\n",
    "                            ]\n",
    "                            )\n",
    "    val_score = cross_val_score(clf_pipeline, sm_xtrain_tfidf, sm_train_y).mean()\n",
    "    print(clf)\n",
    "    print(val_score)\n",
    "    if val_score > best_val_score:\n",
    "        best_val_score = val_score\n",
    "        best_clf = clf\n",
    "    print(\"\\n\")\n",
    "    \n",
    "print('Best:')\n",
    "print(best_val_score)\n",
    "print(best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf_pipeline = Pipeline(\n",
    "                            [\n",
    "                                 (\"classifier\", best_clf())\n",
    "                            ]\n",
    "                        )\n",
    "best_clf_pipeline.fit(xtrain_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actual_Risk_Type = y_test.tolist()\n",
    "Predicted_Risk_Type = (best_clf_pipeline.predict(xtest_tfidf))\n",
    "matches = 0\n",
    "for i in range (len(xtest_tfidf)):\n",
    "    if(Actual_Risk_Type[i] == Predicted_Risk_Type[i]):\n",
    "        matches+=1\n",
    "Accuracy = round(matches/len(xtest_tfidf), 2)\n",
    "print(\"{} is the model accuracy\".format(Accuracy))\n",
    "print(Actual_Risk_Type)\n",
    "print(Predicted_Risk_Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Actual_Risk_Type, Predicted_Risk_Type, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
